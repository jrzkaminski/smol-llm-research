{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZJKgn9Cvk7a",
        "outputId": "5d14d40f-9c9d-4917-9aea-46c71353fbb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Nxjtxl2JK2d4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')#.to(device)\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')#.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/ultratool.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "VhsLFhrUMsu5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "lfqFz7bvvlyE"
      },
      "outputs": [],
      "source": [
        "parsed_data = dict()\n",
        "parsed_tools = dict()\n",
        "\n",
        "for row in data[:100]:\n",
        "    tools = row['tools']\n",
        "    question = row['question']\n",
        "    parsed_data[question] = []\n",
        "    for tool in tools:\n",
        "        name = tool['name']\n",
        "        parsed_data[question].append(name)\n",
        "        if name not in parsed_tools.keys():\n",
        "            description = tool['description']\n",
        "            try:\n",
        "                results = tool['results']['properties']\n",
        "            except KeyError:\n",
        "                parsed_tools[name] = description\n",
        "                continue\n",
        "            results_desc = '. Returns'\n",
        "            for k, v in results.items():\n",
        "                try:\n",
        "                    res_desc = v['description']\n",
        "                except KeyError:\n",
        "                    try:\n",
        "                        for k, v_ in v['properties'].items():\n",
        "                            res_desc = v_['description']\n",
        "                            results_desc += f' {res_desc[0].lower()}{res_desc[1:]},'\n",
        "                    except KeyError:\n",
        "                        for k, v__ in v['items']['properties'].items():\n",
        "                            res_desc = v__['description']\n",
        "                            results_desc += f' {res_desc[0].lower()}{res_desc[1:]},'\n",
        "                results_desc += f' {res_desc[0].lower()}{res_desc[1:]},'\n",
        "            results_desc = results_desc[:-1] + '.'\n",
        "            description += results_desc\n",
        "            parsed_tools[name] = description"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "def get_embeddings(sentences):\n",
        "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    #sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "    return sentence_embeddings\n",
        "\n",
        "def get_cosine_similarity(emb1, emb2):\n",
        "    cos_sim = dot(emb1, emb2)/(norm(emb1)*norm(emb2))\n",
        "    return cos_sim\n",
        "\n",
        "def list_metrics(predict, target):\n",
        "    predict = set(predict)\n",
        "    target = set(target)\n",
        "\n",
        "    intersection = predict.intersection(target)\n",
        "    union = predict.union(target)\n",
        "\n",
        "    tn = len([item for item in predict if item not in target])\n",
        "    fp = len([item for item in target if item not in predict])\n",
        "\n",
        "    intersection_length = len(intersection)\n",
        "    union_length = len(union)\n",
        "\n",
        "    jaccard = intersection_length / union_length\n",
        "\n",
        "    try:\n",
        "        fp_tn = fp / tn\n",
        "    except ZeroDivisionError:\n",
        "        fp_tn = 3\n",
        "\n",
        "    return fp_tn, jaccard"
      ],
      "metadata": {
        "id": "eM5AyveSDMlF"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_emb = get_embeddings(list(parsed_data.keys()))\n",
        "tools_emb = get_embeddings(list(parsed_tools.values()))"
      ],
      "metadata": {
        "id": "DjNsn2JTSooC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 0.4\n",
        "\n",
        "jaccard_total = []\n",
        "fp_tn_total = []\n",
        "\n",
        "for i, (question, used_tools) in enumerate(parsed_data.items()):\n",
        "    predicted_tools = []\n",
        "    for j, (tool_name, tool_description) in enumerate(parsed_tools.items()):\n",
        "        cos_sim = get_cosine_similarity(questions_emb[i], tools_emb[j])\n",
        "        if cos_sim > THRESHOLD:\n",
        "            predicted_tools.append(tool_name)\n",
        "    fp_tn, jaccard = list_metrics(predicted_tools, used_tools)\n",
        "    jaccard_total.append(jaccard)\n",
        "    fp_tn_total.append(fp_tn)\n",
        "\n",
        "print(f'jaccard: {sum(jaccard_total) / len(jaccard_total)}')\n",
        "print(f'fp_tn: {sum(fp_tn_total) / len(fp_tn_total)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9nc8GOgNoNM",
        "outputId": "d3063dec-77f2-4221-d336-4e60b9ee3050"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaccard: 0.3551772116772117\n",
            "fp_tn: 1.5610357142857145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EjrykNBS9b7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(question, tools_descriptions):\n",
        "    sentences = tools_descriptions + [question]\n",
        "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "    return sentence_embeddings[:-1], sentence_embeddings[-1]\n",
        "\n",
        "\n",
        "jaccard_total = []\n",
        "fp_tn_total = []\n",
        "for question, used_tools in parsed_data.items():\n",
        "    predicted_tools = []\n",
        "    tools_emb, question_emb = get_embeddings(question, list(parsed_tools.values()))\n",
        "    for i, (tool_name, tool_description) in enumerate(parsed_tools.items()):\n",
        "        cos_sim = get_cosine_similarity(tools_emb[i], question_emb)\n",
        "        if cos_sim > THRESHOLD:\n",
        "            predicted_tools.append(tool_name)\n",
        "    fp_tn, jaccard = list_metrics(predicted_tools, used_tools)\n",
        "    jaccard_total.append(jaccard)\n",
        "    fp_tn_total.append(fp_tn)\n",
        "\n",
        "print(f'jaccard: {sum(jaccard_total) / len(jaccard_total)}')\n",
        "print(f'fp_tn: {sum(fp_tn_total) / len(fp_tn_total)}')"
      ],
      "metadata": {
        "id": "D2qL_kjjSUxY"
      },
      "execution_count": 51,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}